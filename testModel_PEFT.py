# testModel_PEFT.py
import os
import json
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
import numpy as np
from peft import PeftModel


class EmotionAndEmbeddingModel:
    def __init__(self, path="./results_18emo_output/emotion_model_18emo"):
        """åŠ è½½PEFT-Turned S-BERTæ¨¡å‹"""
        base_model_path = os.path.join(path, "base_model")
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        label_mapping_path = os.path.join(path, "label_mapping.json")
        with open(label_mapping_path, "r", encoding="utf-8") as f:
            label_config = json.load(f)
        self.id2label = label_config["id2label"]
        self.label2id = label_config["label2id"]

        base_model = AutoModelForSequenceClassification.from_pretrained(
            base_model_path,
            num_labels=18,
            id2label=self.id2label,
            label2id=self.label2id,
            ignore_mismatched_sizes=True,  # é˜»æ­¢è‡ªåŠ¨åˆå§‹åŒ–åˆ†ç±»å¤´
        )  # åˆå§‹åŒ–åˆ†ç±»ä»»åŠ¡
        self.model = PeftModel.from_pretrained(base_model, path)  # å°†adapterå’Œåˆ†ç±»å¤´è¦†ç›–åˆ°æ¨¡å‹ä¸Š
        self.model.to(self.device).eval()

        print(self.model)

        print("=" * 20)
        print("\nåŠ è½½çš„æ ‡ç­¾æ˜ å°„å…³ç³»:")
        for id, label in self.id2label.items():
            print(f"{id}: {label}")

    def predict_emotion(self, text, confidence_threshold=0.2) -> dict:
        """å¸¦ç½®ä¿¡åº¦è¿‡æ»¤çš„æƒ…ç»ªé¢„æµ‹"""
        inputs = self.tokenizer(text, truncation=True, max_length=256, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)

        pred_prob, pred_id = torch.max(probs, dim=1)
        pred_prob = pred_prob.item()
        pred_id = pred_id.item()

        top3 = self._get_top3(probs)

        if pred_prob < confidence_threshold:
            return {
                "label": "ä¸ç¡®å®š",
                "confidence": pred_prob,
                "top3": top3,
                "warning": f"ç½®ä¿¡åº¦ä½äºé˜ˆå€¼({confidence_threshold:.0%})",
            }

        return {"label": self.id2label[str(pred_id)], "confidence": pred_prob, "top3": top3}

    def get_embedding(self, sentences, normalize=True):
        """è·å–å¥å­å‘é‡ï¼Œå½“å‰ä¸ºBAAI/bge-base-zh-v1.5ç‰¹åŒ–ç‰ˆ"""

        if isinstance(sentences, str):
            # bge-base-zh-v1.5ä¸ºæ£€ç´¢ä»»åŠ¡åšäº†ç‰¹åŒ–ï¼Œåœ¨å¥å­å¼€å¤´æ·»åŠ  "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š" æ¨¡å‹æ•ˆæœä¼šæ›´å¥½
            # sentences = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š" + sentences
            sentences = [sentences]

        inputs = self.tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors="pt").to(
            self.device
        )
        base_model = self.model.get_base_model()  # è·å¾—åŸå§‹åˆ†ç±»ä»»åŠ¡æ¨¡å‹
        encoder = getattr(base_model, base_model.config.model_type)  # è·å¾—åŸå§‹åŸºåº§ä»»åŠ¡æ¨¡å‹

        with torch.no_grad():
            # ç›´æ¥è°ƒç”¨åŸºåº§æ¨¡å‹çš„ç¼–ç å™¨
            outputs = encoder(**inputs)

        def mean_pooling(model_output, attention_mask):
            token_embeddings = model_output[0]
            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
                input_mask_expanded.sum(1), min=1e-9
            )

        # bge-base-chinese-v1.5ä½¿ç”¨ CLS Pooling
        last_hidden_state = outputs.last_hidden_state
        cls_embedding = last_hidden_state[:, 0]
        if normalize:
            cls_embedding = F.normalize(cls_embedding, p=2, dim=1)

        # text2vec-base-chineseé‡‡ç”¨ mean pooling
        # cls_embedding = mean_pooling(outputs, inputs['attention_mask'])

        return cls_embedding.cpu().numpy()

    def _get_top3(self, probs) -> list:
        top3_probs, top3_ids = torch.topk(probs, 3)
        return [
            {"label": self.id2label[str(idx.item())], "probability": prob.item()}
            for prob, idx in zip(top3_probs[0], top3_ids[0])
        ]


def handle_embedding_comparison(model_handler):
    print("\nè¿›å…¥å¥å­ç›¸ä¼¼åº¦æ¯”è¾ƒæ¨¡å¼")
    try:
        source_sentence = input("è¯·è¾“å…¥æºå¥å­: ").strip()
        if not source_sentence:
            print("æºå¥å­ä¸èƒ½ä¸ºç©ºï¼")
            return

        target_sentences = []
        print("è¯·è¾“å…¥ç›®æ ‡å¥å­ï¼Œæ¯è¡Œä¸€ä¸ªã€‚å½“ä¸è¾“å…¥ä»»ä½•å†…å®¹å¹¶å›è½¦æ—¶ç»“æŸ:")
        while True:
            line = input().strip()
            if not line:
                break
            target_sentences.append(line)

        if not target_sentences:
            print("æ²¡æœ‰è¾“å…¥ä»»ä½•ç›®æ ‡å¥å­ï¼Œæ“ä½œå–æ¶ˆã€‚")
            return

        print("\næ­£åœ¨è®¡ç®—ç›¸ä¼¼åº¦...")

        all_sentences = [source_sentence] + target_sentences
        all_embeddings = model_handler.get_embedding(all_sentences)

        source_embedding = all_embeddings[0]
        target_embeddings = all_embeddings[1:]

        # (N, D) * (D,) -> (N,)  Nç›®æ ‡å¥å­æ•°, Då‘é‡ç»´åº¦
        similarities = np.dot(target_embeddings, source_embedding)

        results = sorted(zip(target_sentences, similarities), key=lambda item: item[1], reverse=True)

        print("\n" + "=" * 40)
        print(f"æºå¥å­: '{source_sentence}'")
        print("ç›¸ä¼¼åº¦æ’åºç»“æœ:")
        print("-" * 20)
        for i, (sentence, score) in enumerate(results):
            print(f"{i+1}. å¾—åˆ†: {score:.4f} | å¥å­: {sentence}")
        print("=" * 40)

    except Exception as e:
        print(f"\nâŒ åœ¨æ¯”è¾ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


def main():
    print("ã€æƒ…ç»ªåˆ†æ & å¥å‘é‡ç”Ÿæˆå™¨ã€‘")
    print("=" * 40)

    try:
        model_handler = EmotionAndEmbeddingModel()
        print("\næ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print("è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†æã€‚")
        print("è¾“å…¥ ':q' é€€å‡ºã€‚")
        print("è¾“å…¥ ':embed <æ–‡æœ¬>' è·å–å¥å‘é‡ã€‚")
        print("è¾“å…¥ ':compare' æˆ– ':sim' è¿›å…¥ç›¸ä¼¼åº¦æ¯”è¾ƒæ¨¡å¼ã€‚")
        print("=" * 40)
    except Exception as e:
        print(f"\næ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæ–‡ä»¶æ˜¯å¦å®Œæ•´ã€‚")
        return

    while True:
        try:
            text_input = input("\nè¯·è¾“å…¥æŒ‡ä»¤æˆ–æ–‡æœ¬: ").strip()

            if text_input.lower() in [":q", ":quit", "exit"]:
                print("\né€€å‡ºç¨‹åº")
                break

            if not text_input:
                print("è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼")
                continue

            if text_input.lower().startswith(":embed "):
                text_to_embed = text_input[len(":embed ") :].strip()
                if not text_to_embed:
                    print("è¯·è¾“å…¥éœ€è¦ç¼–ç çš„æ–‡æœ¬å†…å®¹ï¼")
                    continue
                embedding = model_handler.get_embedding(text_to_embed)
                print(f"å‘é‡ç»´åº¦: {embedding.shape}, é¢„è§ˆ: {embedding[0][:5]}...")

            elif text_input.lower() in [":compare", ":sim"]:
                handle_embedding_comparison(model_handler)

            else:
                print("\n" + "=" * 30)
                print("ğŸ¯ æ¨¡å¼: æƒ…æ„Ÿåˆ†ç±»")
                print(f"ğŸ“ æ–‡æœ¬: {text_input}")
                result = model_handler.predict_emotion(text_input)

                if "warning" in result:
                    print(f"âš ï¸ {result['warning']}")
                print(f"ä¸»æƒ…ç»ª: {result['label']} (ç½®ä¿¡åº¦: {result['confidence']:.2%})")

                if result["label"] != "ä¸ç¡®å®š":
                    print("\nå…¶ä»–å¯èƒ½æƒ…ç»ª:")
                    for i, item in enumerate(result["top3"][1:], 1):
                        print(f"{i}. {item['label']}: {item['probability']:.2%}")
                print("=" * 30)

        except KeyboardInterrupt:
            print("\næ£€æµ‹åˆ°ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº...")
            break
        except Exception as e:
            print(f"\nâŒ é¢„æµ‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main()
